{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='../../Data/Extracted/'\n",
    "\n",
    "############\n",
    "### Here load condensed text data if required ###\n",
    "### Get the code for that from the last cell ###\n",
    "#############\n",
    "\n",
    "train_text=pd.read_csv(path+'train_multi_column_text.csv')\n",
    "test_text=pd.read_csv(path+'test_multi_column_text.csv')\n",
    "\n",
    "train_num=pd.read_csv(path+'train.csv').select_dtypes(exclude='object')\n",
    "train_num.fillna(train_num.median(),inplace=True)\n",
    "\n",
    "test_num=pd.read_csv(path+'test.csv').select_dtypes(exclude='object')\n",
    "test_num.fillna(test_num.median(),inplace=True)\n",
    "\n",
    "############\n",
    "### Here load binned numeric data if required ###\n",
    "### Get the code for that from the last cell ###\n",
    "#############\n",
    "\n",
    "labels=pd.read_csv(path+'labels.csv')\n",
    "labels=pd.get_dummies(labels,prefix_sep='__')\n",
    "#for feat in labels.columns:\n",
    "#    labels[feat],_=pd.factorize(labels[feat],sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scaling numeric data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nomalize train and test data sets\n",
    "train_num=preprocessing.normalize(train_num)\n",
    "test_num=preprocessing.normalize(test_num)\n",
    "\n",
    "# scale and shift the data set with standard scaler\n",
    "std=preprocessing.StandardScaler()\n",
    "train_num=std.fit_transform(train_num)\n",
    "test_num=std.transform(test_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vectorize train and test text by TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csr=[train_num]\n",
    "test_csr=[test_num]\n",
    "\n",
    "############\n",
    "### Here tfidf vectorize single column text vector if required ###\n",
    "### Get the code for that from the last cell ###\n",
    "############\n",
    "\n",
    "for feat in tqdm(train_text.columns):\n",
    "    vectorizer=TfidfVectorizer(ngram_range=(1,4),min_df=10)\n",
    "    train_csr.append(vectorizer.fit_transform(train_text[feat].values.ravel()))\n",
    "    test_csr.append(vectorizer.transform(test_text[feat].values.ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load target encoding of the categorical features from the extracted folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "### Here load target encoding if required ###\n",
    "### Get the code for that from the last cell ###\n",
    "############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=scipy.sparse.hstack(train_csr).tocsr()\n",
    "X_test=scipy.sparse.hstack(test_csr).tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train simple online logistic regression with SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "import os\n",
    "\n",
    "#https://www.kaggle.com/c/instacart-market-basket-analysis/discussion/37753#\n",
    "def iter_minibatches(chunksize, X,y):\n",
    "    # Provide chunks one by one\n",
    "    chunk_start_marker = 0\n",
    "    while chunk_start_marker < X.shape[0]:\n",
    "        chunkrows = range(chunk_start_marker, min(chunk_start_marker + chunksize,X.shape[0]))\n",
    "        # you need to implement \"getrows\" (based on what your data source is - RAM/Disc/...)\n",
    "        X_chunk, y_chunk = X[chunkrows],y[chunkrows]\n",
    "        yield X_chunk, y_chunk # get next portion of data to train on\n",
    "        chunk_start_marker += chunksize # update starting point\n",
    "\n",
    "score=0\n",
    "for label in tqdm(labels.columns):\n",
    "    # label as a target feature for the current model\n",
    "    y_train=labels[label].values\n",
    "                        \n",
    "    # Path to store best model while training and early stopping\n",
    "    path='./best_model/'+label\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    # train test split to evaluate model performance on the cv data set to decide early stopping\n",
    "    X_tr,X_cv,y_tr,y_cv=model_selection.train_test_split(X_train,y_train,test_size=0.2,stratify=y_train,random_state=44)\n",
    "\n",
    "    # model to be train\n",
    "    clf=linear_model.SGDClassifier(loss='log',\n",
    "                                 eta0=0.001,\n",
    "                                 validation_fraction=0.2,\n",
    "                                 early_stopping=False,\n",
    "                                 n_jobs=-1) # Estimator\n",
    "    \n",
    "    improvement=[10e10] # Track cv score while fitting the model\n",
    "    epoch=0\n",
    "    cnt=0\n",
    "    patience=10 # Number of epochs to wait without improvement in cv loss\n",
    "    max_epochs=1000\n",
    "    tol=0.001\n",
    "    \n",
    "    # Train while we hit our patience level and trigger early stopping or we reach max epoch number\n",
    "    while cnt<patience and epoch<max_epochs:\n",
    "\n",
    "        # Use batcheterator for mini-batch SGD\n",
    "        batcheterator=iter_minibatches(10000,X_tr,y_tr)\n",
    "        min_loss=np.min(improvement) # Note min_loss for each epoch\n",
    "        for x_chunk,y_chunk in batcheterator:\n",
    "            clf.partial_fit(x_chunk,y_chunk,classes=np.unique(y_train))\n",
    "        improvement.append(metrics.log_loss(y_cv,clf.predict_proba(X_cv)))\n",
    "        curr_loss=improvement[-1]\n",
    "        \n",
    "        if (min_loss-curr_loss)>tol:\n",
    "            # If current loss is less than the current minimum loss then this is our best model so far\n",
    "\n",
    "            np.save(path+'/best_prediction.npy',[val[1] for val in clf.predict_proba(X_test)])\n",
    "            cnt=0\n",
    "        else:\n",
    "            # Model did not improve\n",
    "            cnt+=1\n",
    "\n",
    "        epoch+=1\n",
    "    score+=np.min(improvement)\n",
    "print('score:',score/9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create submission file in the Submissions folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read submission format file\n",
    "submission_format=pd.read_csv('../../Data/Original/SubmissionFormat.csv')\n",
    "# Save index for submission\n",
    "index=(submission_format[submission_format.columns[0]].tolist())\n",
    "submission_format=submission_format[submission_format.columns[1:]]\n",
    "\n",
    "# Create submission dictionary\n",
    "path='best_model/'\n",
    "sub_dict={}\n",
    "for col in submission_format.columns:\n",
    "    sub_dict[col]=np.load(path+col+'/best_prediction.npy')\n",
    "    \n",
    "# Make submission pandas and save it as csv file\n",
    "sub_df=pd.DataFrame(sub_dict,columns=submission_format.columns,index=index)\n",
    "sub_df.to_csv('../../Data/Submissions/logistic_104_models_target_encoding_loo_8.csv',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "### Here make submission file for 9 models if required ###\n",
    "### Get the code for that from the last cell ###\n",
    "#############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Things that didn't work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1) Use condensed text feature\n",
    "\n",
    "1.1 To load condensed text feature\n",
    "\n",
    "    train_text=pd.read_csv(path+'train_condensed_text.csv')\n",
    "    test_text=pd.read_csv(path+'test_condensed_text.csv')\n",
    "\n",
    "2) Use original numeric data\n",
    "\n",
    "2.1 if want load original numeric data from the train and test csv files from the extracted folder\n",
    "\n",
    "    train_num=pd.read_csv(path+'train.csv').select_dtypes(exclude='object')\n",
    "    train_num.fillna(train_num.median(),inplace=True)\n",
    "\n",
    "    test_num=pd.read_csv(path+'test.csv').select_dtypes(exclude='object')\n",
    "    test_num.fillna(test_num.median(),inplace=True)\n",
    "    \n",
    "\n",
    "2.2 to scale original numeric features normalization and standardization\n",
    "\n",
    "    # Nomalize train and test data sets\n",
    "    train_num=preprocessing.normalize(train_num)\n",
    "    test_num=preprocessing.normalize(test_num)\n",
    "\n",
    "    # scale and shift the data set with standard scaler\n",
    "    std=preprocessing.StandardScaler()\n",
    "    train_num=std.fit_transform(train_num)\n",
    "    test_num=std.transform(test_num)\n",
    "    \n",
    "\n",
    "3) load binned numeric data\n",
    "\n",
    "3.1 train_num=pd.read_csv(path+'train_binned_numeric_data.csv')\n",
    "    test_num=pd.read_csv(path+'test_binned_numeric_data.csv')\n",
    "\n",
    "4) Vectorize single column text vector\n",
    "\n",
    "4.1 vectorizer=TfidfVectorizer(ngram_range=(1,4),min_df=10)\n",
    "    train_csr.append(vectorizer.fit_transform(train_text.values.ravel()))\n",
    "    test_csr.append(vectorizer.transform(test_text.values.ravel()))\n",
    "\n",
    "\n",
    "5) Use entropy based features\n",
    "    \n",
    "5.1 if want to load entrop features read features \n",
    "\n",
    "    path='../../Data/Extracted/'\n",
    "    train_entropy=pd.read_csv(path+'train_feature_label_entropy.csv')\n",
    "    test_entropy=pd.read_csv(path+'test_feature_label_entropy.csv')\n",
    "    \n",
    "5.2 Then add following in the prediction cell\n",
    "\n",
    "    # Create new X_train (currently list) for a current label\n",
    "    X_train=train_csr.copy()\n",
    "    \n",
    "    # Append numpy array of the entropy based features for the current label\n",
    "    X_train.append(np.array(train_entropy[list(filter(lambda val:val.startswith(label),train_entropy.columns))]))\n",
    "    \n",
    "    # Convert X_train into a sparse matrix after hstacking all elements of the list\n",
    "    X_train=scipy.sparse.hstack(X_train).tocsr()\n",
    "\n",
    "5.3 and for test add\n",
    "\n",
    "    # Create new X_test (currently list) for this label\n",
    "    X_test=test_csr.copy()\n",
    "    # Same as for training set, add entropy based features for the current label into our list\n",
    "    X_test.append(np.array(test_entropy[list(filter(lambda val:val.startswith(label),test_entropy.columns))]))\n",
    "    # Convert X_test list into sparse matrix after hstacking all elements of the list\n",
    "    X_test=scipy.sparse.hstack(X_test).tocsr()\n",
    "    \n",
    "6) Target encoding\n",
    "\n",
    "    path='../../Data/Extracted/'\n",
    "\n",
    "    with open(path+'train_cat_feat_target_encoding.pickle', 'rb') as f:\n",
    "        train_feat_enc=pickle.load(f)\n",
    "\n",
    "    with open(path+'test_cat_feat_target_encoding.pickle', 'rb') as f:\n",
    "        test_feat_enc=pickle.load(f)\n",
    "\n",
    "    for label in labels.columns:\n",
    "        train_csr.append(train_feat_enc[label])\n",
    "        test_csr.append(test_feat_enc[label])\n",
    "\n",
    "7) Submission for 9 models\n",
    "\n",
    "    submission_format=pd.read_csv('../../Data/Original/SubmissionFormat.csv')\n",
    "    # Save index for submission\n",
    "    index=(submission_format[submission_format.columns[0]].tolist())\n",
    "    submission_format=submission_format[submission_format.columns[1:]]\n",
    "\n",
    "    path='best_model'\n",
    "    best_predictions=[]\n",
    "    for label in os.listdir(path):\n",
    "        best_predictions.append(np.load(path+'/'+label+'/best_prediction.npy'))\n",
    "\n",
    "    best_predictions=np.hstack(best_predictions)\n",
    "\n",
    "    logistic_submission=pd.DataFrame(best_predictions,columns=submission_format.columns,index=index)\n",
    "\n",
    "    logistic_submission.to_csv('../../Data/Submissions/logistic_model_more_drop_feats_104_7.csv',index=True)\n",
    "    \n",
    "8) Adding target encoded features\n",
    "\n",
    "    # Initialize train and test lists\n",
    "    X_train=train_csr.copy()\n",
    "    X_test=test_csr.copy()\n",
    "    \n",
    "    # Append target encoded features into X_train and X_test\n",
    "    X_train.append(np.array(train_feat_enc[label]))\n",
    "    X_test.append(np.array(test_feat_enc[label]))\n",
    "    \n",
    "    # Create sparse matrix after hstack\n",
    "    X_train=scipy.sparse.hstack(X_train).tocsr()\n",
    "    X_test=scipy.sparse.hstack(X_test).tocsr()\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
